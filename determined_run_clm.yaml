name: llm-fine-tuning
workspace: llm-fine-tuning
project: japanese-gpt-neox-small
environment:
  environment_variables:
    - NCCL_DEBUG=INFO
    - http_proxy=http://hpeproxy.its.hpecorp.net:8080/
    - https_proxy=http://hpeproxy.its.hpecorp.net:8080/
    - no_proxy=localhost,127.0.0.1,ponkots01,16.171.32.68,10.0.0.0/8,192.168.0.0/16,172.16.0.0/16
  image:
    cpu: localhost:32000/llm-fine-tuning:latest
    gpu: localhost:32000/llm-fine-tuning:latest
  force_pull_image: false
bind_mounts:
  - host_path: /data/microk8s/jhub-claim-sugiyama-pvc-695191f5-690d-4804-a951-f2de3df6aa50/llm-fine-tuning/outputs
    container_path: outputs
  - host_path: /data/home/sugiyama/.cache
    container_path: /root/.cache
  - host_path: /home/sugiyama/.netrc
    container_path: /root/.netrc
resources:
  slots_per_trial: 8
  shm_size: 274877906944
max_restarts: 0
# hyperparameters:
#   training_arguments:
#     learning_rate:
#       type: log
#       base: 10
#       minval: -5
#       maxval: -3
#     per_device_train_batch_size:
#       type: int
#       minval: 2
#       maxval: 5
searcher:
  name: single
  # name: adaptive_asha
  max_length:
    epochs: 1
    # batches: 900
  metric: eval_loss
  # max_trials: 32
  # max_concurrent_trials: 1
  # mode: aggressive
entrypoint: >-
  python -m determined.launch.torch_distributed
  python run_clm.py
  --model_name_or_path rinna/japanese-gpt-neox-small
  --dataset_name llm-book/livedoor-news-corpus
  --num_train_epochs 1
  --per_device_train_batch_size 1
  --per_device_eval_batch_size 1
  --do_train
  --do_eval
  --fp16
  --block_size 1024
  --deepspeed ds_config_zero2.json
  --output_dir outputs/test-clm
  --overwrite_output_dir
  --report_to wandb
  --evaluation_strategy steps
  --eval_steps 100
  --logging_strategy steps
  --logging_steps 100
  --save_strategy no
